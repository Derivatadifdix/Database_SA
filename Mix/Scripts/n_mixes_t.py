# -*- coding: utf-8 -*-
"""N_mixes_T.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/189QBafNQL9qz_SlT7hfnvOp90e9cvVQ_
"""

# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FfbDH8Xd8wbgwjYFB6jjgyKKfk-SlT_M
"""

import pandas as pd
import platform
platform.platform()

"""# DOWNLOAD THE DATASET AND UNZIP IT"""

!wget http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip -P /content/datasets/

#!unzip /content/datasets/stanfordSentimentTreebank.zip -d /content/datasets/

!ls datasets/stanfordSentimentTreebank

"""# CREATE THE DATAFRAMES"""

# Load sentence data
sentences_dt = pd.read_csv("/content/datasets/stanfordSentimentTreebank/datasetSentences.txt",
                                                                        sep = "\t",
                                                                        header=0,
                                                                        names=['sentence_index','sentence'])
sentences_dt

# Load sentiment labels data :
#contains all phrase ids and the corresponding sentiment labels

sentiment_labels_dt = pd.read_csv("/content/datasets/stanfordSentimentTreebank/sentiment_labels.txt",
                                  sep="|",
                                  header=0,
                                  names=["phrase_id", "sentiment_label"])

sentiment_labels_dt

#Load dictionary.txt
#contains all phrases and their IDs
phrases_ds = pd.read_csv("/content/datasets/stanfordSentimentTreebank/dictionary.txt",
                         sep = "|",
                         names=["phrase", "phrase_id"])
phrases_ds= phrases_ds[["phrase_id", "phrase"]]
phrases_ds

phrases_df = phrases_ds.set_index("phrase_id").join(sentiment_labels_dt.set_index("phrase_id"), how="inner")

phrases_df = phrases_df.reset_index()
phrases_df

"""#DEFINE THE DATA AND TARGET SETS

"""

x = phrases_df[["phrase"]]
xn = x
y = phrases_df[['phrase_id','sentiment_label']]
y.loc[:4]

x

"""# REMOVE THE STOP WORDS FROM X"""

with open("/content/datasets/stopword.txt") as fil:
  f = fil.read().replace("\n", "")
stop_words = f.split(",")

stop_words_df = pd.DataFrame(data=stop_words, columns=['STOP_WORD'])
stop_words_df

#Lower the strings
x_copy = x.copy()
x_copy["phrase"] = x_copy["phrase"].str.lower()
x = x_copy

print("Shape of x is:", x.shape[0], "\n")
#Remove Nan raws
x.dropna(subset=["phrase"], inplace=True)

print("Shape of x is:", x.shape[0], "\n")

#Removing the stopwords in x
x = x_copy[~x_copy['phrase'].isin(stop_words)]
print("Shape of x is:", x.shape[0], "\n")

x['phrase_id']= phrases_df['phrase_id']
print(y.columns)

"""#Remove from target y the rows removed from x"""

#we want len(x)==len(y) for training
y_copy = y.copy()
print("len y: ", len(y))
y = y_copy[y_copy['phrase_id'].isin(x['phrase_id'])]
#print(len(y)==len(x)) #239230

"""# CONVERT THE TARGET INTO DISCRETE LABELS

"""

categories = {0.2 :'very negative' , 0.4:'negative', 0.6:'neutral', 0.8:'positive', 1:'very positive'}

thresholds = list(categories.keys())

y["disc_sentiment_label"] = y["sentiment_label"].apply(
                                                      lambda x:
                                                      next(categ for threshold, categ in categories.items()
                                                                  if x<=threshold),
                                                      "outlier")

y.loc[:15]

"""#TRAINING OF THE MODEL"""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x['phrase'],
                                                    y["disc_sentiment_label"],
                                                    test_size=0.3,
                                                    random_state=42)

from sklearn.feature_extraction.text import TfidfVectorizer

vect = TfidfVectorizer(max_features=10000)

vect_x_train = vect.fit_transform(x_train)
vect_x_test = vect.transform(x_test)

print("Len:", len(y_train), vect_x_train.shape[0], len(x_test), len(y_test))

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

model = LogisticRegression(max_iter=10000)
model.fit(vect_x_train, y_train)

predictions = model.predict(vect_x_test)
acc = accuracy_score(y_test, predictions)

print("Acurracy: {:0.3f}".format(acc))

"""#TEST ON PHRASE"""

ph = pd.Series("it way cool!")
ph_target_pd = pd.Series("very positive")
v_ph = vect.transform(ph)

print("the prediction is: ", model.predict(v_ph))

"""# **FINE TUNE WITH TWITTER MODEL**"""

tweets = pd.read_csv("/content/datasets/tweets.csv",
                     sep=",",
                     names=['sentiment_label', 'id', 'date', 'flag', 'user', 'text'],
                     on_bad_lines='skip',
                     encoding='latin_1')

tweets

"""Redefine sentimet labels"""

def categorize_sentiment(x):
  if x == 4:
    return 'positive'
  if x == 0:
    return 'negative'

tweets['disc_sentiment_label'] = tweets['sentiment_label'].apply(categorize_sentiment)
tweets

"""NEW TRAINING SETS"""

x = tweets['text']
y = tweets['sentiment_label']